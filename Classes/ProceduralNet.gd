class_name ProceduralNet extends Node

'''
Инициализация: Задаем начальные значения весов
θ (например, случайно).

Цикл обучения (эпоха/итерация):
	Прямой проход (Forward):
		Подаем данные x.
		Вычисляем предсказание сети y1 и функцию потерь L(θ),
		сохраняя все промежуточные значения.

	Обратное распространение (Backward):
		Используя сохраненные промежуточные значения и правило цепочки,
		вычисляем градиент функции потерь по всем весам сети: ∇L(θ).

	Шаг градиентного спуска (Gradient Descent Step):
		Обновляем все веса сети, используя вычисленный градиент:
		θ = θ − α∇L(θ).
'''

## Входной слой сети.
var init_layer: Array[Neuron]

var d
